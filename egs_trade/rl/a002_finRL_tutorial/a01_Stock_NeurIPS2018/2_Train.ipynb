{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "uijiWgkuh1jB",
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 安装依赖包，导入头文件\n",
    "\n",
    "本样例复现论文：*Practical Deep Reinforcement Learning Approach for Stock Trading （https://arxiv.org/abs/1811.07522）*.\n",
    "\n",
    "代码参考：[FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))"
   ],
   "metadata": {
    "id": "gT-zXutMgqOS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "原理介绍：\n",
    "\n",
    "强化学习核心部分包括“机器人”和“环境”。流程大致如下：\n",
    "- 机器人和环境进行交互，观察到当前的条件，称为“状态”（**state**），并且可以执行“动作”（**action**）\n",
    "- 机器人执行动作后，会进入一个新的状态，同时，环境给机器人一个反馈，叫奖励（**reward**）\n",
    "  (通过数字反馈新状态的好坏)\n",
    "- 之后，机器人和环境不停的重复交互，机器人要尽可能多的获取累计奖励\n",
    "\n",
    "强化学习是一种方法，让机器人学会提升表现，并达成目标。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "实现介绍\n",
    "\n",
    "使用OpenAI gym的格式构建股票交易的环境。\n",
    "\n",
    "state-action-reward的含义如下：\n",
    "\n",
    "- **State s**: 状态空间表示机器人对环境的感知。就像人工交易员分析各种信息和数据。机器人从历史数据\n",
    "  观察交易价格以及技术指标。通过和环境交互进行学习（一般通过回放历史数据）\n",
    "  \n",
    "- **Action a**: 动作空间代码机器人在每个状态可以执行的动作。例如，a ∈ {−1, 0, 1}, −1, 0, 1代表\n",
    "  卖出、持仓、买入。当处理多支股票时，a ∈{−k, ..., −1, 0, 1, ..., k}, 比如，“买10股AAPL”或者\n",
    "  “卖出10股AAPL”即10或-10。\n",
    "\n",
    "- **Reward function r(s, a, s′)**: 奖励用于激励机器人学习一个更好的策略。例如，在状态s下执行动作a\n",
    "  以改变投资组合值，并到达一个新的状态s', 例如，r(s, a, s′) = v′ − v, v′ 和 v 代表状态分别在s′ \n",
    "  和s时的投资组合总市值。\n",
    "  \n",
    "- **Market environment**: 道琼斯工业平均指数（DJIA）中30只成分股，包含回测时间段的所有交易数据。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 如果没有安装，解注释进行安装\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "## 请把下面解注释，安装finrl库\n",
    "# 或者把如下Github仓中的finrl文件夹考到根目录即可使用\n",
    "##!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
    "\n",
    "# 强化学习库，使用stable_baselines3\n",
    "# 注意：\n",
    "#    1. 强化学习比起机器学习慢很多，CPU训练大约2分钟，强化学习使用单卡GPU大约需要30分钟左右完成训练\n",
    "#    2. 强化学习不稳定，每次收敛的loss不一样，且效果可能差异大     "
   ],
   "metadata": {
    "id": "D0vEcPxSJ8hI",
    "pycharm": {
     "is_executing": false
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.meta.data_processor import DataProcessor"
   ],
   "metadata": {
    "id": "xt1317y2ixSS",
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T09:57:30.145574Z",
     "start_time": "2024-11-05T09:57:30.128616Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from finrl import config  # 包含各类超参\n",
    "from finrl import config_tickers  # 常见各类市场股票代码集合，比如中证300\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "# 创建目录\n",
    "check_and_make_directories([TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ],
   "metadata": {
    "id": "wZ7Bl7i6I2AM",
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T09:57:33.409361Z",
     "start_time": "2024-11-05T09:57:33.400382Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 在OpenAI Gym-style构建市场环境"
   ],
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 读取训练数据，所有股票均混在了一个csv表里，格式如下\n",
    "# 索引     日期          股票\n",
    "#  0      2009-01-02    苹果\n",
    "#  0      2009-01-02    亚马逊\n",
    "#  1      2009-01-05    苹果\n",
    "#  1      2009-01-05    亚马逊\n",
    "\n",
    "# 注意：必须保持上述该格式，同样的索引下至少有2个数据，否则会报错，\n",
    "# 原因：\n",
    "#   1. 在finrl/meta/env_stock_trading/env_stocktrading.py的\n",
    "#      _initiate_state函数中self.data.close.values.tolist()，\n",
    "#      在404行，要求self.data.close必须是二维数组\n",
    "#   2. 而finrl/meta/env_stock_trading/env_stocktrading.py的\n",
    "#      __init__的64行self.data = self.df.loc[self.day, :]，\n",
    "#      如果索引顺序排，0，1，2。。。，会导致只取到一个行数，一维\n",
    "#      数据传入导致第1点中所述的问题\n",
    "#      （因此，如果只有一支股票时，需要把索引全部改成一样的，当然\n",
    "#      这种情况几乎不存在，也可以暂时忽略）\n",
    "# 解决方法：\n",
    "# 1. 降低numpy版本\n",
    "# 2. 把数据改成二维的，即（10，）-》（1，10） （改完是否存在回测不完整性，没有详细验证）\n",
    "# 3. 保持最上方所示的数据格式（推荐）\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_SAVE_DIR, 'train_data.csv'))\n",
    "train = train.set_index(train.columns[0]) # 第一列为索引\n",
    "train.index.names = ['']\n",
    "assert train.shape[0] > 1, '数据必须至少包含2行，即2天以上'"
   ],
   "metadata": {
    "id": "mFCP1YEhi6oi",
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T09:58:05.101027Z",
     "start_time": "2024-11-05T09:58:04.855431Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 默认定义了8个技术因子\n",
    "INDICATORS"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwk32SeKJGWZ",
    "outputId": "258b8796-6fd7-445e-eb4a-d964597d248b",
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T09:58:19.474567Z",
     "start_time": "2024-11-05T09:58:19.459607Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['macd',\n 'boll_ub',\n 'boll_lb',\n 'rsi_30',\n 'cci_30',\n 'dx_30',\n 'close_30_sma',\n 'close_60_sma']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array(['AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM', 'CSCO', 'CVX', 'DIS',\n       'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM',\n       'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT'],\n      dtype=object)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tic.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-05T09:59:03.941840Z",
     "start_time": "2024-11-05T09:59:03.931866Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# 共29支股票，状态空间291\n",
    "stock_dimension = len(train.tic.unique())\n",
    "# 状态说明\n",
    "# 1：账户余额\n",
    "# [1: stock_dimension+1]: 股票价格\n",
    "# [stock_dimension+1: 1 + 2*stock_dimension]: 持仓数量\n",
    "# len(INDICATORS)*stock_dimension：每支股票的因子状态，bool型表示\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "1455279f-d280-4a4f-a555-935fadd2bdb7",
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T10:02:40.268035Z",
     "start_time": "2024-11-05T10:02:40.251080Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension  # 手续费\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ],
   "metadata": {
    "id": "WsOLoeNcJF8Q",
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T10:06:09.138001Z",
     "start_time": "2024-11-05T10:06:09.112071Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 构建训练环境"
   ],
   "metadata": {
    "id": "7We-q73jjaFQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "35605c17-bdda-4f30-86bd-6db9b80c1f1e",
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T10:06:30.449593Z",
     "start_time": "2024-11-05T10:06:30.434629Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# 3. 训练深度强化学习模型\n",
    "* 强化学习库：使用 **Stable Baselines 3**. 也可以尝试更换 **ElegantRL** and **Ray RLlib**.\n",
    "* FinRL库包含精调的标准深度强化学习算法, 包括DQN, DDPG, Multi-Agent DDPG, PPO, SAC, A2C and TD3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "364PsqckttcQ",
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T10:20:27.512842Z",
     "start_time": "2024-11-05T10:20:27.494890Z"
    }
   },
   "outputs": [],
   "source": [
    "# 选择需要使用的强化学习算法\n",
    "\n",
    "# 5种算法：A2C, DDPG, PPO, TD3, SAC\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = False\n",
    "if_using_td3 = False\n",
    "if_using_sac = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "## Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GUCnkn-HIbmj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "af2f8c1e-a300-4dc4-fe3d-84861bb4606d",
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-05T10:29:27.800682Z",
     "start_time": "2024-11-05T10:20:29.741217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to results/a2c\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -0.817    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 9.56      |\n",
      "|    reward             | 0.1462618 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.531     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.0243     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 21.6       |\n",
      "|    reward             | -1.9981467 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.01       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 92       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -338     |\n",
      "|    reward             | 4.996992 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 67.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.0131    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 108        |\n",
      "|    reward             | -0.3330955 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.00693  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 360       |\n",
      "|    reward             | -9.520514 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 102       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -1.17     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 225       |\n",
      "|    reward             | 0.5379273 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 32.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 4.35e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -60.2     |\n",
      "|    reward             | -3.192931 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.07e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -40.7      |\n",
      "|    reward             | -3.0260303 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.36       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 122        |\n",
      "|    reward             | -1.1795219 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 11         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 50.5      |\n",
      "|    reward             | -4.097868 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.61      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 94       |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 58       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -240     |\n",
      "|    reward             | 3.859386 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 33.3     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 92          |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 64          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | -0.00434    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -101        |\n",
      "|    reward             | -0.19830866 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 7.63        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 46.1       |\n",
      "|    reward             | -3.3095126 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.8        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 75         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -0.00442   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 72.5       |\n",
      "|    reward             | 0.74298143 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.95       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 93       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.5    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -140     |\n",
      "|    reward             | 2.014509 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 21       |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 467       |\n",
      "|    reward             | 0.8822583 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 128       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 93          |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 91          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | -0.362      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 100         |\n",
      "|    reward             | -0.61032695 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 28.9        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0.114     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 57.1      |\n",
      "|    reward             | 2.8127394 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.53      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 101       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -0.162    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 8.21      |\n",
      "|    reward             | 1.0078067 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.01      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 107        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -19.2      |\n",
      "|    reward             | 0.43951124 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.443      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 93       |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 112      |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 84.8     |\n",
      "|    reward             | 2.029274 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 8.88     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 91.5      |\n",
      "|    reward             | -8.823501 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 9.68      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 122        |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -1.83e+03  |\n",
      "|    reward             | -1.1760488 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.03e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0.0186     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 88.2       |\n",
      "|    reward             | 0.41637453 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 10.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -68.7      |\n",
      "|    reward             | 0.16197458 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.94       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -4.31     |\n",
      "|    reward             | 1.6475976 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.0589    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 144        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -81.5      |\n",
      "|    reward             | -0.7429423 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.06       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 628       |\n",
      "|    reward             | 2.2245374 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 298       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -93.7     |\n",
      "|    reward             | 1.4511429 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.82      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 160       |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | 66.5      |\n",
      "|    reward             | 0.7186623 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.17      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 93          |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 165         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 0.0169      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 93.9        |\n",
      "|    reward             | -0.41759107 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 5.47        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 170        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -84.3      |\n",
      "|    reward             | -3.1068995 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 10.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 93          |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 176         |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | -120        |\n",
      "|    reward             | -0.18438491 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 11.9        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 93       |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -5.27    |\n",
      "|    reward             | 3.332038 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 10.4     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 93          |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 186         |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 182         |\n",
      "|    reward             | -0.19939433 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 22.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 191       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -130      |\n",
      "|    reward             | 2.3051596 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 12.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 197       |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 224       |\n",
      "|    reward             | 1.9869581 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 31        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 202       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -0.064    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 173       |\n",
      "|    reward             | 1.2153405 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 19.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 207       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -76.7     |\n",
      "|    reward             | 1.0584443 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.32      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 213       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -0.000338 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -97.2     |\n",
      "|    reward             | 1.8412559 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 14.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 218        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -8.58e-06  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -43.9      |\n",
      "|    reward             | 0.20718887 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.83       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 224       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0.0825    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | -301      |\n",
      "|    reward             | 0.3272537 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 62.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 229       |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0.0723    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -200      |\n",
      "|    reward             | 6.1550236 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 23.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 234       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0.0439    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 59.7      |\n",
      "|    reward             | 3.4964473 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.1       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 240       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 553       |\n",
      "|    reward             | 1.7721924 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 202       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 93       |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 245      |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 154      |\n",
      "|    reward             | 7.103068 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 19.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 250        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -132       |\n",
      "|    reward             | -0.0861763 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 15.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 255        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -104       |\n",
      "|    reward             | -1.3617942 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 261        |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | -62.2      |\n",
      "|    reward             | 0.19459197 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 266        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -0.0745    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 63.6       |\n",
      "|    reward             | -1.9005316 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.71       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 271        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | -161       |\n",
      "|    reward             | -2.5921288 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 37.2       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 93       |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 276      |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -572     |\n",
      "|    reward             | 3.962138 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 257      |\n",
      "------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5037170.44\n",
      "total_reward: 4037170.44\n",
      "total_cost: 26409.62\n",
      "total_trades: 54019\n",
      "Sharpe: 0.926\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 281       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -47.4     |\n",
      "|    reward             | 0.9252851 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.26      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 94          |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 287         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0.00213     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | -6.28       |\n",
      "|    reward             | -0.03348774 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 4.54        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 292        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -108       |\n",
      "|    reward             | 0.71700436 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 24.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 94          |\n",
      "|    iterations         | 5600        |\n",
      "|    time_elapsed       | 297         |\n",
      "|    total_timesteps    | 28000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5599        |\n",
      "|    policy_loss        | -191        |\n",
      "|    reward             | -0.43553826 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 27.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 302        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -65.8      |\n",
      "|    reward             | -2.5296538 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.06       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 308        |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -0.0138    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | -57.7      |\n",
      "|    reward             | 0.96653795 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 5.7        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 94          |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 313         |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | 25.7        |\n",
      "|    reward             | -0.51569337 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.803       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 94          |\n",
      "|    iterations         | 6000        |\n",
      "|    time_elapsed       | 318         |\n",
      "|    total_timesteps    | 30000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5999        |\n",
      "|    policy_loss        | 22.6        |\n",
      "|    reward             | -0.06800026 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.68        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 323        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | 3.09       |\n",
      "|    reward             | -1.9623663 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 329       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -68.2     |\n",
      "|    reward             | -0.741805 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 334       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 51.3      |\n",
      "|    reward             | 1.8995097 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.69      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 339       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 126       |\n",
      "|    reward             | 1.4536406 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 344        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 39.9       |\n",
      "|    reward             | -4.6971636 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 7.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 350        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -65.2      |\n",
      "|    reward             | -0.2587001 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.34       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 355       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0.0388    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -717      |\n",
      "|    reward             | -5.771268 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 360       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 94          |\n",
      "|    iterations         | 6800        |\n",
      "|    time_elapsed       | 360         |\n",
      "|    total_timesteps    | 34000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6799        |\n",
      "|    policy_loss        | -200        |\n",
      "|    reward             | -0.03159203 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 24.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 365       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | 336       |\n",
      "|    reward             | 3.3240962 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 100       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 371       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 126       |\n",
      "|    reward             | 0.6900647 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 10.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 376       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 87.1      |\n",
      "|    reward             | 1.5771554 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.8       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 94          |\n",
      "|    iterations         | 7200        |\n",
      "|    time_elapsed       | 381         |\n",
      "|    total_timesteps    | 36000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7199        |\n",
      "|    policy_loss        | -371        |\n",
      "|    reward             | -0.15649427 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 75.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 386       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | 143       |\n",
      "|    reward             | 2.4556105 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 15.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 392        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | 26.8       |\n",
      "|    reward             | -6.3326116 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 8.5        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 397        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -6.55      |\n",
      "|    reward             | -4.6129227 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 402       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0.00359   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -156      |\n",
      "|    reward             | 2.7124765 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 14.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 407       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -111      |\n",
      "|    reward             | 1.2205862 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 8.74      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 413        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 0.937      |\n",
      "|    reward             | 0.17544279 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.975      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 94       |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 418      |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | 248      |\n",
      "|    reward             | 3.845736 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 45.6     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 424        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -34.2      |\n",
      "|    reward             | -0.8081745 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.84       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 94       |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 429      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -9.81    |\n",
      "|    reward             | 5.136959 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 7.15     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 434        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | 25.4       |\n",
      "|    reward             | 0.33643386 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.798      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 440       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -0.034    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 56.3      |\n",
      "|    reward             | -1.726232 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.03      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 446        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -0.00922   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -41.5      |\n",
      "|    reward             | -3.4824119 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.04       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 94          |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 451         |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -145        |\n",
      "|    reward             | 0.056321163 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 14.8        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 456        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | -0.0206    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 250        |\n",
      "|    reward             | -11.411316 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 48.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 94        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 462       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -19.5     |\n",
      "|    reward             | 0.9257479 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 2.21      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 94         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 467        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | -58.4      |\n",
      "|    reward             | 0.70410514 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 2.95       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 473        |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | 17.2       |\n",
      "|    reward             | 0.18703595 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.33       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 478        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 18.5       |\n",
      "|    reward             | -0.7331002 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 5.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 484       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | 28.1      |\n",
      "|    reward             | 3.2931387 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 3.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 490       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -72.8     |\n",
      "|    reward             | 3.5570848 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 495       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -190      |\n",
      "|    reward             | 0.5394218 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 19.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 501        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 139        |\n",
      "|    reward             | -0.6459062 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 12.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 507        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 114        |\n",
      "|    reward             | 0.45486665 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 512        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0.000235   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -525       |\n",
      "|    reward             | 0.10857164 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 154        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 519       |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 71.2      |\n",
      "|    reward             | -1.498569 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 4.68      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 93       |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 525      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -46.7    |\n",
      "|    reward             | 4.590423 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 42       |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 93          |\n",
      "|    iterations         | 9900        |\n",
      "|    time_elapsed       | 531         |\n",
      "|    total_timesteps    | 49500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 0.0167      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9899        |\n",
      "|    policy_loss        | -1.85       |\n",
      "|    reward             | -0.32636908 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.322       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 536        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 12.2       |\n",
      "|    reward             | -1.1015437 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.436      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\blueluelue\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path 'trained_models\\a2c' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "if if_using_a2c:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/a2c'\n",
    "    new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_a2c.set_logger(new_logger_a2c)\n",
    "    \n",
    "    trained_a2c = agent.train_model(model=model_a2c, \n",
    "                                 tb_log_name='a2c',\n",
    "                                 total_timesteps=50000)\n",
    "\n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'a2c') \n",
    "    trained_a2c.save(os.path.join(save_path, \"agent_a2c.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "## Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "M2YadjfnLwgt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "90405544-f219-431a-8fe4-76bebba1be54",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "if if_using_ddpg:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/ddpg'\n",
    "    new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_ddpg.set_logger(new_logger_ddpg)\n",
    "    \n",
    "    trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) \n",
    "    \n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'ddpg') \n",
    "    trained_ddpg.save(os.path.join(save_path, \"agent_ddpg.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y5D5PFUhMzSV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f378334f-833f-4ec1-e2ba-f12a0bdebf70",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "if if_using_ppo:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/ppo'\n",
    "    new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_ppo.set_logger(new_logger_ppo)\n",
    "    \n",
    "    trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000)\n",
    "    \n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'ppo') \n",
    "    trained_ppo.save(os.path.join(save_path, \"agent_ppo.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JSAHhV4Xc-bh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "283fa6a7-1997-4816-8582-a9fcdcfa73d0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "\n",
    "if if_using_td3:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/td3'\n",
    "    new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_td3.set_logger(new_logger_td3)\n",
    "    \n",
    "    trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000)\n",
    "    \n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'td3') \n",
    "    trained_td3.save(os.path.join(save_path, \"agent_td3\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xwOhVjqRkCdM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f7688355-6ae8-483c-ec33-d202bcab702e",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to results\\sac\n",
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4068845.14\n",
      "total_reward: 3068845.14\n",
      "total_cost: 21184.91\n",
      "total_trades: 43718\n",
      "Sharpe: 0.759\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 33         |\n",
      "|    time_elapsed    | 348        |\n",
      "|    total_timesteps | 11572      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 368        |\n",
      "|    critic_loss     | 151        |\n",
      "|    ent_coef        | 0.0967     |\n",
      "|    ent_coef_loss   | -107       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 11471      |\n",
      "|    reward          | -3.0241768 |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 707       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 177       |\n",
      "|    critic_loss     | 33.7      |\n",
      "|    ent_coef        | 0.0307    |\n",
      "|    ent_coef_loss   | -144      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 23043     |\n",
      "|    reward          | 2.0108583 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5350106.89\n",
      "total_reward: 4350106.89\n",
      "total_cost: 10445.33\n",
      "total_trades: 51816\n",
      "Sharpe: 0.889\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 1056     |\n",
      "|    total_timesteps | 34716    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 93.5     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    ent_coef        | 0.00984  |\n",
      "|    ent_coef_loss   | -155     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 34615    |\n",
      "|    reward          | 2.77428  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 1403     |\n",
      "|    total_timesteps | 46288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 60.2     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.00334  |\n",
      "|    ent_coef_loss   | -78.7    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 46187    |\n",
      "|    reward          | 3.231979 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 1741      |\n",
      "|    total_timesteps | 57860     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 37.9      |\n",
      "|    critic_loss     | 5.65      |\n",
      "|    ent_coef        | 0.00139   |\n",
      "|    ent_coef_loss   | 2.27      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 57759     |\n",
      "|    reward          | 3.1411095 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5133030.87\n",
      "total_reward: 4133030.87\n",
      "total_cost: 1418.53\n",
      "total_trades: 49490\n",
      "Sharpe: 0.858\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 2079     |\n",
      "|    total_timesteps | 69432    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.4     |\n",
      "|    critic_loss     | 4.37     |\n",
      "|    ent_coef        | 0.00157  |\n",
      "|    ent_coef_loss   | 4.93     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 69331    |\n",
      "|    reward          | 6.714324 |\n",
      "---------------------------------\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 在单卡GPU下, batch调大显存占用和利用率也上不去，不确定原因\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "if if_using_sac:\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "    # set up logger\n",
    "    tmp_path = os.path.join(RESULTS_DIR, 'sac') \n",
    "    new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    # Set new logger\n",
    "    model_sac.set_logger(new_logger_sac)\n",
    "    \n",
    "    trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000)\n",
    "\n",
    "    save_path = os.path.join(TRAINED_MODEL_DIR, 'sac') \n",
    "    trained_sac.save(os.path.join(save_path, 'agent_sac.zip'))"
   ]
  }
 ]
}
